# ML - Portfolio Pitch: Mila-Assist - Version Enrichie

## Projet

- Nom: Mila-Assist ‚Äî Assistant virtuel IA pour streameurs (RAG + base vectorielle)
- Auteur:
- Lead Developer & ML Engineer (architecture RAG, base de donn√©es vectorielle): Samuel VERSCHUEREN
- Backend Developer (API REST, s√©curit√©): Samuel VERSCHUEREN
- Frontend Developer (interface utilisateur, UX): Samuel VERSCHUEREN
- Data Engineer (preprocessing, optimisation des mod√®les): Samuel VERSCHUEREN

## Introduction

Mila-Assist est un assistant virtuel con√ßu pour les streameurs. Il s'appuie sur une **architecture hybride multi-mod√®les** combinant une recherche s√©mantique (TF‚ÄëIDF + similarit√© cosinus), un syst√®me de fallback avec r√©seau de neurones **Keras**, et une reformulation avanc√©e avec **Sentence Transformers**. Objectifs: fournir de l'aide √† la configuration et r√©duire la charge de l'assistance pour les d√©veloppeurs de l'IA AI_Licia.

## Architecture IA - Mod√®les et Transformers

### üß† Mod√®le Principal: chatbot_model.keras

**Type**: R√©seau de neurones dense (Sequential) pour classification d'intentions  
**Framework**: TensorFlow/Keras 2.7.0  
**Taille**: ~330 KB (mod√®le optimis√© pour CPU)

#### Architecture du mod√®le:
```
Input Layer ‚Üí Dense(128, ReLU) ‚Üí Dropout(0.5) 
           ‚Üí Dense(64, ReLU) ‚Üí Dropout(0.5) 
           ‚Üí Dense(45, Softmax)
```

#### Caract√©ristiques techniques:
- **Entr√©e**: Vectorisation Bag-of-Words (vocabulaire lemmatis√© de ~800 mots)
- **Classes de sortie**: 45 intents sp√©cialis√©s (greetings, configuration, troubleshooting, etc.)
- **Optimiseur**: SGD avec d√©croissance exponentielle du learning rate (0.01 ‚Üí 0.96^epoch)
- **Dropout**: 50% pour √©viter le surapprentissage
- **Activation**: ReLU pour les couches cach√©es, Softmax pour la classification

#### Pourquoi ce mod√®le ?
1. **Rapidit√©**: Inf√©rence < 50ms sur CPU (i5, 8GB RAM)
2. **Robustesse**: Test√© sur 45 cat√©gories avec 200+ patterns d'entra√Ænement
3. **Fallback fiable**: Garantit une r√©ponse m√™me si la base vectorielle √©choue
4. **L√©g√®ret√©**: Pas de d√©pendance GPU, compatible NAS/VPS

### üîÑ Transformer: paraphrase-multilingual-MiniLM-L12-v2

**Type**: Sentence Transformer multilingue  
**Mod√®le de base**: Microsoft MiniLM-L12  
**Usage**: Mode "naturel" pour reformulation contextuelle avanc√©e

#### Sp√©cifications:
- **Taille**: ~130 MB (mod√®le compact optimis√©)
- **Langues**: Support fran√ßais/anglais natif
- **Dimensions**: 384-dimensional sentence embeddings
- **Performance**: ~200-400ms par requ√™te sur CPU

#### Pourquoi ce transformer ?
1. **Multilingual**: Optimis√© pour le fran√ßais (important pour le public streameur francophone)
2. **Compact**: MiniLM-L12 vs mod√®les lourds (BERT-Large: 1.3GB)
3. **Paraphrase-optimized**: Sp√©cialement entra√Æn√© pour la reformulation de phrases
4. **Production-ready**: Stable avec sentence-transformers 2.2.2

### üèóÔ∏è Architecture Hybride - Cascade Intelligente

```
Requ√™te utilisateur
        ‚Üì
1Ô∏è‚É£ Recherche vectorielle TF-IDF (SQL + cosinus)
   ‚îú‚îÄ Similarit√© > 0.7 ‚Üí R√©ponse directe
   ‚îî‚îÄ √âchec ‚Üì
2Ô∏è‚É£ Mod√®le Keras (chatbot_model.keras)
   ‚îú‚îÄ Probabilit√© > 0.25 ‚Üí Classification intent
   ‚îî‚îÄ √âchec ‚Üì
3Ô∏è‚É£ R√©ponse par d√©faut

Reformulation finale selon le mode:
‚îú‚îÄ MINIMAL: R√©ponse brute
‚îú‚îÄ BALANCED: Templates + variations
‚îî‚îÄ NATURAL: Sentence Transformer + contexte conversationnel
```

#### Justification de l'approche hybride:
1. **Performance**: TF-IDF pour 80% des cas courants (ultra-rapide)
2. **Robustesse**: Keras pour les cas complexes (fallback intelligent)
3. **Qualit√©**: Sentence Transformer pour l'exp√©rience utilisateur premium
4. **√âvolutivit√©**: Ajout facile de nouveaux mod√®les sans casser l'existant

### üìä Vectorisation et Embedding

#### TF-IDF (Primary):
- **Algorithme**: Term Frequency - Inverse Document Frequency
- **Impl√©mentation**: scikit-learn TfidfVectorizer
- **Stockage**: Base SQLite avec index optimis√©s
- **Performance**: ~10-30ms par requ√™te

#### Sentence Embeddings (Enhanced):
- **M√©thode**: Transformations contextuelles via MiniLM
- **Utilisation**: Reformulation et continuit√© conversationnelle
- **Cache**: Embeddings mis en cache pour optimisation

## Description ‚Äî Exp√©rience utilisateur

- Interface web (Flask) simple pour poser des questions et obtenir des r√©ponses instantan√©es.
- API REST s√©curis√©e (cl√© API) pour int√©gration avec OBS/StreamerBot.
- **Mode adaptatif**: Le syst√®me choisit automatiquement le mod√®le optimal selon les ressources disponibles
- **3 modes de qualit√©**:
  - **Minimal**: TF-IDF pur (< 50ms, 50MB RAM)
  - **Balanced**: TF-IDF + templates (< 100ms, 100MB RAM)
  - **Natural**: TF-IDF + Sentence Transformers (< 400ms, 200MB RAM)
- Gain de temps et Disponibilit√© 24/24

## Donn√©es

- Types: questions/patterns, tags/intents, r√©ponses, feedbacks, journaux d'interactions, scores de similarit√©.
- Collecte:
  - Import automatis√© depuis intents.json (legacy).
  - Migration intelligente vers base vectorielle TF-IDF
  - Feedbacks saisis via l'UI/API, avec quotas pour √©viter l'abus.
- Stockage:
  - SQLite (dev/local) ou MySQL (prod/NAS) via SQLAlchemy.
  - Embeddings: TF‚ÄëIDF persist√©, recalcul p√©riodique.
  - **Mod√®les persist√©s**: chatbot_model.keras + vocabulaire (words.pkl, classes.pkl)
  - Logs: fichiers ou table d√©di√©e.

### üîÑ Pipeline de donn√©es IA:
```
intents.json ‚Üí Preprocessing (NLTK) ‚Üí 
‚îú‚îÄ TF-IDF Vectorization ‚Üí SQLite/MySQL
‚îú‚îÄ Keras Training Data ‚Üí chatbot_model.keras
‚îî‚îÄ Vocabulary ‚Üí words.pkl + classes.pkl
```

## Choix Techniques et Justifications IA

### Pourquoi ne pas utiliser un LLM complet ?
1. **Ressources limit√©es**: D√©ploiement sur NAS Synology (ARM, 2-4GB RAM)
2. **Co√ªt**: Pas d'API externe payante (OpenAI, Claude)
3. **Latence**: < 500ms requis pour l'exp√©rience streameur
4. **Contr√¥le**: R√©ponses pr√©dictibles et mod√©rables

### Pourquoi TensorFlow/Keras vs PyTorch ?
1. **√âcosyst√®me mature**: TF 2.7 stable pour production
2. **Optimisation CPU**: TensorFlow Lite compatible si besoin
3. **D√©ploiement**: Support Docker + serving simple
4. **Legacy**: Compatibilit√© avec mod√®le existant v1.0

### Pourquoi MiniLM vs autres transformers ?
1. **Taille**: 130MB vs 1.3GB (BERT-Large) vs 500MB (CamemBERT)
2. **Multilingual**: Fran√ßais natif vs mod√®les anglais uniquement
3. **Paraphrase**: Sp√©cialis√© reformulation vs classification g√©n√©rale
4. **CPU Performance**: Optimis√© pour inf√©rence sans GPU

## √âthique et conformit√©

- Pas de donn√©es personnelles stock√©es.
- **Mod√®les locaux**: Aucune donn√©e envoy√©e vers API externes
- Anonymisation des feedbacks; rate‚Äëlimit + mod√©ration basique pour √©viter les abus.
- **Transparence**: README d√©taill√© + architecture ouverte
- Droit √† l'oubli: aucune donn√©e utilisateur n'est stock√©e
- **Auditabilit√©**: Logs des pr√©dictions et scores de confiance

## Plateformes cibl√©es

- D√©veloppement: Linux/Windows, CPU only (min i5, 8 Go RAM).
- D√©ploiement: Docker, NAS Synology/VPS, API HTTP.
- **Mod√®les optimis√©s**: Pas de d√©pendance GPU; mod√®les compacts privil√©gi√©s.
- **Monitoring IA**: M√©triques de performance des mod√®les int√©gr√©es

## Contraintes et hypoth√®ses

- Ressources limit√©es: **Choix d√©lib√©r√© de mod√®les l√©gers** vs gros LLM/embeddings lourds.
- **Compromis qualit√©/performance**: TF‚ÄëIDF + Keras vs Transformers lourds
- S√©curit√©: acc√®s API restreint par cl√©.
- Disponibilit√©: **Syst√®me de fallback en cascade** pour assurer r√©ponse m√™me si composants IA √©chouent.

## MVP (fonctionnalit√©s c≈ìur)

- **Recherche hybride**: TF‚ÄëIDF + similarit√© cosinus sur base vectorielle (SQLite).
- **Fallback intelligent**: intents.json + mod√®le Keras si score < seuil.
- **API REST s√©curis√©e** (cl√© API) + interface web.
- **Modes adaptatifs**: D√©tection automatique des ressources disponibles

## Feature cibl√©e (scope court)

- Refondre la gestion des retours utilisateurs:
  - **Am√©lioration des mod√®les via feedback**: R√©entra√Ænement incr√©mental
  - Am√©lioration de l'interface graphique par rapport √† la version 1
  - Version du tchat en .exemples
  - Ajout du tchat sur le site
  - **Int√©gration semi‚Äëauto**: Suggestions de nouveaux patterns/r√©ponses bas√©es sur l'analyse IA

## Donn√©es ‚Äî D√©tails de la feature

- Sch√©ma feedback:
  - id, timestamp, source (web/api), ip_hash, tag_candidat, message_utilisateur, reponse_fournie, commentaire.
  - **Scores de confiance**: M√©triques IA pour qualit√© des pr√©dictions
- R√®gles anti‚Äëabus: rate‚Äëlimit par IP, seuils journaliers, blocage si spam d√©tect√©.
- Respect vie priv√©e: ip_hash par HMAC(secret, ip), rotation possible.
- **Am√©lioration continue**: Feedback utilis√© pour fine-tuning des mod√®les

## M√©triques et Monitoring IA

### üìà KPIs Techniques:
- **Latence par mod√®le**: TF-IDF (<30ms), Keras (<50ms), Sentence-T (<400ms)
- **Taux de couverture**: % requ√™tes r√©solues par niveau (vectoriel/keras/fallback)
- **Pr√©cision par intent**: Accuracy du mod√®le Keras par cat√©gorie
- **Similarit√© moyenne**: Score cosinus pour recherche vectorielle

### üîç Monitoring Production:
- **Health checks**: V√©rification disponibilit√© mod√®les au d√©marrage
- **Performance logs**: Temps de r√©ponse par composant IA
- **Error tracking**: √âchecs de pr√©diction et fallbacks activ√©s
- **Resource usage**: RAM/CPU par mod√®le en temps r√©el

## Planification (4 semaines)

- S1 ‚Äî Mise en service du chatbot (priorit√© au fonctionnement)
  - V√©rifier l'installation, .env et initialisation DB; (re)construction de l'index TF‚ÄëIDF.
  - **Validation mod√®les IA**: Test chargement Keras + Sentence Transformers
  - Assurer le pipeline de r√©ponse: UI/endpoint de question ‚Üí recherche TF‚ÄëIDF ‚Üí fallback intents ‚Üí r√©ponse.
  - Ajuster le seuil de similarit√© et corriger les erreurs bloquantes; tests manuels de bout en bout.
  - UI minimale op√©rationnelle (champ de saisie + affichage de la r√©ponse).

- S2 ‚Äî Enrichissement base + tests basiques
  - Collecter manuellement des Q/R pertinentes depuis le Discord officiel (sans donn√©es personnelles).
  - **Optimisation mod√®les**: Fine-tuning seuils TF-IDF + Keras confidence
  - Normaliser et ins√©rer dans la knowledge_base (tags, question canonique, r√©ponse); recalcul TF‚ÄëIDF.
  - Ajuster les seuils pour am√©liorer la pertinence CPU‚Äëonly.
  - Ajouter quelques tests unitaires **+ tests de performance IA**.

- S3 ‚Äî Enrichissement avanc√©
  - Continuer l'enrichissement Q/R depuis Discord, cat√©goriser par tags, puis recalcul TF‚ÄëIDF.
  - **Int√©gration Sentence Transformers**: Mode naturel + reformulation contextuelle
  - Cr√©er une table feedbacks minimale: id, timestamp, question, rating ‚àà {‚àí1,0,+1}, reponse_id|tag, commentaire optionnel.
  - **M√©triques IA**: Dashboard monitoring performance mod√®les
  - UI: am√©lioration globale.
  - Ne pas stocker de message utilisateur; conserver uniquement des m√©triques (rating/tag/horodatage).
  - Nettoyage de code (organisation fichiers, PEP8).

- S4 ‚Äî Finition et d√©mo
  - **Optimisation finale**: Cache embeddings + compression mod√®les
  - Nettoyage de code (organisation fichiers, PEP8).
  - Mise √† jour du README (captures, variables .env, endpoints, **architecture IA**, limites et choix √©thiques).
  - **Vid√©o d√©mo**: question ‚Üí r√©ponse ‚Üí 3 modes IA ‚Üí ajout Q/R.
  - Demander la QA manuelle et pr√©parer 3‚Äì4 slides de pr√©sentation **+ m√©triques IA**.

## Risques et mitigations

- Manque de temps: concentrer sur **mod√®les core** (TF-IDF + Keras) + API + UI, reporter Sentence Transformers avanc√©.
- **Ressources limit√©es**: Benchmark continu, fallback obligatoire, monitoring RAM/CPU
- **Performance IA**: D√©gradation gracieuse entre modes, cache intelligent
- S√©curit√©: cl√©s API, rate‚Äëlimit, logs d'audit; **pas de PII dans les mod√®les**.

## Indicateurs de succ√®s (KPIs)

- **Couverture intelligente**: > 85% requ√™tes r√©solues par TF-IDF (niveau 1)
- **Pr√©cision Keras**: > 75% accuracy sur validation set
- **Performance**: < 500ms temps de r√©ponse moyen (tous modes)
- Taux de feedback utile (> 70%).
- R√©duction des questions sans r√©ponse.
- **Latence IA**: TF-IDF < 30ms, Keras < 50ms, Sentence-T < 400ms
- Augmentation de la couverture de la base de connaissances.

## D√©pendances

### üîß Core IA:
- **TensorFlow 2.7.0**: Mod√®le Keras + optimisations CPU
- **sentence-transformers 2.2.2**: MiniLM-L12 multilingue  
- **scikit-learn 1.0.1**: TF-IDF vectorization + similarit√© cosinus
- **nltk 3.6.5**: Preprocessing (tokenization, lemmatization)
- **numpy**: Calculs matriciels optimis√©s

### üåê Infrastructure:
- Python 3.9+, Flask, SQLAlchemy
- Optionnel: Docker, MySQL, reverse proxy (Caddy/Nginx).

## D√©mo

- **Web**: interface Flask + monitoring IA temps r√©el
- **API**: /api/ask, /api/feedback, /api/model_status
- **Donn√©es**: intents.json migr√©es + base vectorielle TF-IDF + mod√®le Keras
- **Modes**: D√©monstration des 3 niveaux de qualit√© IA

## Blog post

**Titre**: ¬´ Construire un assistant streameur intelligent : Architecture hybride TF-IDF + Keras + Sentence Transformers (CPU-only) ¬ª

**Sections**: 
1. **Contexte**: Pourquoi pas un LLM complet ?
2. **Architecture IA**: Cascade TF-IDF ‚Üí Keras ‚Üí Transformers
3. **Choix techniques**: Mod√®les l√©gers vs performance
4. **Optimisations CPU**: Latence < 500ms sans GPU  
5. **S√©curit√© & feedback**: Am√©lioration continue des mod√®les
6. **D√©mo**: 3 modes de qualit√© en action
7. **Le√ßons**: Compromis intelligents pour contraintes mat√©rielles

---

## üöÄ Points diff√©renciants ML/IA

### Innovation technique:
1. **Architecture cascade intelligente**: Optimisation ressources via fallbacks
2. **Mod√®les adaptatifs**: Auto-s√©lection selon ressources disponibles  
3. **Hybrid RAG**: Recherche vectorielle + classification neuronale
4. **CPU-optimized**: Production sans GPU sur hardware limit√©

### Valeur ajout√©e:
- **D√©ploiement accessible**: NAS domestique vs cloud co√ªteux
- **R√©ponses contr√¥l√©es**: Pr√©dictibilit√© vs hallucinations LLM
- **Am√©lioration continue**: Feedback loop pour fine-tuning
- **Multilingual native**: Fran√ßais optimis√© vs adaptation mod√®les anglais

Cette approche d√©montre une **ma√Ætrise technique des contraintes r√©elles** de d√©ploiement IA en production avec des ressources limit√©es, tout en maintenant une exp√©rience utilisateur de qualit√©.