#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TRAIN_V2.PY - ENTRA√éNEMENT UNIFI√â POUR MILA ASSIST (VERSION RNCP6)
========================================================================

- Gestion intelligente des backups (limitation √† 3 versions)
- Suppression automatique des anciennes sauvegardes
- M√©triques d'entra√Ænement avanc√©es
- Logging am√©lior√© avec historique
- Validation crois√©e optionnelle
- Monitoring des performances

Fonctionnalit√©s:
- R√©cup√©ration des donn√©es depuis l'API/base de donn√©es
- Entra√Ænement du mod√®le Keras pour le fallback local
- Mise √† jour automatique de words.pkl et classes.pkl
- Sauvegarde et versioning intelligent des mod√®les
- Architecture robuste avec gestion d'erreurs

Auteur: Samuel VERSCHUEREN
Date: 16-09-2025
"""

import os
import sys
import json
import pickle
import random
import shutil
import requests
import urllib3
import numpy as np
import nltk
import logging
import time
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from collections import Counter
from dataclasses import dataclass, asdict
from pathlib import Path
from dotenv import load_dotenv

# Imports TensorFlow avec gestion d'erreur
try:
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.regularizers import l2
    TENSORFLOW_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå TensorFlow non disponible: {e}")
    print("üí° Installez avec: pip install tensorflow")
    TENSORFLOW_AVAILABLE = False

# Imports NLTK avec gestion d'erreur
try:
    from nltk.stem import WordNetLemmatizer
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    print("‚ùå NLTK non disponible")
    print("üí° Installez avec: pip install nltk")
    NLTK_AVAILABLE = False

# D√©sactiver les avertissements SSL pour le NAS
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Charger la configuration depuis .env
load_dotenv()

@dataclass
class TrainingMetrics:
    """M√©triques d'entra√Ænement pour suivi des performances"""
    start_time: float
    end_time: float = 0.0
    total_knowledge: int = 0
    valid_tags: int = 0
    vocabulary_size: int = 0
    total_patterns: int = 0
    model_accuracy: float = 0.0
    training_loss: float = 0.0
    epochs_completed: int = 0
    data_augmentation_factor: float = 0.0
    
    @property
    def duration(self) -> float:
        return self.end_time - self.start_time if self.end_time > 0 else 0.0
    
    def to_dict(self) -> dict:
        return {
            **asdict(self),
            'duration_seconds': self.duration,
            'timestamp': datetime.now().isoformat()
        }

class TrainingLogger:
    """Logger centralis√© pour l'entra√Ænement avec historique"""
    
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        # Configuration du logging
        timestamp = datetime.now().strftime('%d-%m-%Y_%Hh%Mmin%Ss')
        log_file = self.log_dir / f"training_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"üöÄ D√©marrage du logging d'entra√Ænement v2.0")
    
    def log_metrics(self, metrics: TrainingMetrics):
        """Sauvegarde les m√©triques d'entra√Ænement avec historique"""
        metrics_file = self.log_dir / "training_metrics_history.json"
        
        try:
            # Charger l'historique existant
            if metrics_file.exists():
                with open(metrics_file, 'r', encoding='utf-8') as f:
                    all_metrics = json.load(f)
            else:
                all_metrics = []
            
            # Ajouter les nouvelles m√©triques
            all_metrics.append(metrics.to_dict())
            
            # Limiter l'historique (garder les 50 derniers entra√Ænements)
            if len(all_metrics) > 50:
                all_metrics = all_metrics[-50:]
            
            # Sauvegarder
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(all_metrics, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"üìä M√©triques sauvegard√©es: pr√©cision={metrics.model_accuracy:.4f}, dur√©e={metrics.duration:.2f}s")
            
            # Nettoyage des anciens logs (garder 10 fichiers)
            self._cleanup_old_logs()
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur sauvegarde m√©triques: {e}")
    
    def _cleanup_old_logs(self):
        """Nettoie les anciens fichiers de log"""
        try:
            log_files = sorted([f for f in self.log_dir.glob("training_*-*-*_*h*min*s.log")], 
                             key=lambda x: x.stat().st_mtime, reverse=True)
            
            # Garder seulement les 10 plus r√©cents
            for old_log in log_files[10:]:
                old_log.unlink()
                self.logger.info(f"üóëÔ∏è Ancien log supprim√©: {old_log.name}")
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur nettoyage logs: {e}")

class BackupManager:
    """Gestionnaire de sauvegardes intelligent avec limitation automatique"""
    
    def __init__(self, base_dir: str, max_backups: int = 3):
        self.base_dir = Path(base_dir)
        self.backup_dir = self.base_dir / "data" / "Backup"
        self.max_backups = max_backups
        self.logger = logging.getLogger(__name__)
        self._create_backup_structure()
    
    def _create_backup_structure(self):
        """Cr√©e la structure des r√©pertoires de sauvegarde"""
        backup_dirs = [
            "Model", "Words", "Classes", "Patterns", "Metrics"
        ]
        
        for dir_name in backup_dirs:
            dir_path = self.backup_dir / dir_name
            dir_path.mkdir(parents=True, exist_ok=True)
            
        self.logger.info(f"üìÅ Structure de backup cr√©√©e dans {self.backup_dir}")
    
    def _get_backup_files(self, backup_type: str, pattern: str) -> List[Path]:
        """R√©cup√®re la liste des fichiers de backup d'un type donn√©"""
        backup_path = self.backup_dir / backup_type
        return sorted(backup_path.glob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)
    
    def _cleanup_old_backups(self, backup_type: str, pattern: str):
        """Supprime les anciens backups au-del√† de la limite"""
        backup_files = self._get_backup_files(backup_type, pattern)
        
        if len(backup_files) > self.max_backups:
            files_to_remove = backup_files[self.max_backups:]
            
            for old_file in files_to_remove:
                try:
                    old_file.unlink()
                    self.logger.info(f"üóëÔ∏è Ancien backup supprim√©: {old_file.name}")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erreur suppression {old_file.name}: {e}")
            
            self.logger.info(f"üßπ Nettoyage {backup_type}: {len(files_to_remove)} fichiers supprim√©s")
    
    def backup_model(self) -> bool:
        """Sauvegarde le mod√®le existant avec gestion automatique des versions"""
        model_path = self.base_dir / "chatbot_model.keras"
        
        if not model_path.exists():
            self.logger.info("‚ÑπÔ∏è Aucun mod√®le existant √† sauvegarder")
            return False
        
        timestamp = datetime.now().strftime("%d-%m-%Y_%Hh%Mmin%Ss")
        backup_path = self.backup_dir / "Model" / f"chatbot_model_{timestamp}.keras"
        
        try:
            shutil.copy2(model_path, backup_path)
            self.logger.info(f"üíæ Mod√®le sauvegard√©: {backup_path.name}")
            
            # Nettoyage automatique
            self._cleanup_old_backups("Model", "chatbot_model_*.keras")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur sauvegarde mod√®le: {e}")
            return False
    
    def backup_vocabulary_files(self) -> bool:
        """Sauvegarde words.pkl et classes.pkl avec gestion des versions"""
        timestamp = datetime.now().strftime("%d-%m-%Y_%Hh%Mmin%Ss")
        success = True
        
        files_to_backup = [
            ("words.pkl", "Words"),
            ("classes.pkl", "Classes")
        ]
        
        for filename, backup_type in files_to_backup:
            source_path = self.base_dir / filename
            
            if source_path.exists():
                backup_path = self.backup_dir / backup_type / f"{filename}_{timestamp}"
                
                try:
                    shutil.copy2(source_path, backup_path)
                    self.logger.info(f"üíæ {filename} sauvegard√©: {backup_path.name}")
                    
                    # Nettoyage automatique
                    self._cleanup_old_backups(backup_type, f"{filename}_*")
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Erreur sauvegarde {filename}: {e}")
                    success = False
            else:
                self.logger.info(f"‚ÑπÔ∏è {filename} n'existe pas encore")
        
        return success
    
    def backup_training_patterns(self, donnees_tags: Dict[str, Any]) -> bool:
        """Sauvegarde les patterns d'entra√Ænement avec m√©tadonn√©es"""
        timestamp = datetime.now().strftime("%d-%m-%Y_%Hh%Mmin%Ss")
        
        # Pr√©parer les donn√©es avec m√©tadonn√©es
        backup_data = {
            "timestamp": timestamp,
            "total_tags": len(donnees_tags),
            "total_patterns": sum(len(d['patterns']) for d in donnees_tags.values()),
            "total_responses": sum(len(d['responses']) for d in donnees_tags.values()),
            "data": donnees_tags
        }
        
        backup_path = self.backup_dir / "Patterns" / f"training_patterns_{timestamp}.pkl"
        
        try:
            with open(backup_path, 'wb') as f:
                pickle.dump(backup_data, f)
            
            self.logger.info(f"üíæ Patterns sauvegard√©s: {backup_path.name}")
            
            # Nettoyage automatique
            self._cleanup_old_backups("Patterns", "training_patterns_*.pkl")
            
            # Mise √† jour du fichier actuel
            current_path = self.base_dir / "training_patterns.pkl"
            with open(current_path, 'wb') as f:
                pickle.dump(donnees_tags, f)
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur sauvegarde patterns: {e}")
            return False
    
    def backup_metrics(self, metrics: TrainingMetrics) -> bool:
        """Sauvegarde les m√©triques d'entra√Ænement"""
        timestamp = datetime.now().strftime("%d-%m-%Y_%Hh%Mmin%Ss")
        backup_path = self.backup_dir / "Metrics" / f"training_metrics_{timestamp}.json"
        
        try:
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(metrics.to_dict(), f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üìä M√©triques sauvegard√©es: {backup_path.name}")
            
            # Nettoyage automatique
            self._cleanup_old_backups("Metrics", "training_metrics_*.json")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur sauvegarde m√©triques: {e}")
            return False
    
    def get_backup_summary(self) -> Dict[str, int]:
        """Retourne un r√©sum√© des sauvegardes disponibles"""
        summary = {}
        
        backup_types = [
            ("Model", "chatbot_model_*-*-*_*h*min*s.keras"),
            ("Words", "words.pkl_*-*-*_*h*min*s"),
            ("Classes", "classes.pkl_*-*-*_*h*min*s"),
            ("Patterns", "training_patterns_*-*-*_*h*min*s.pkl"),
            ("Metrics", "training_metrics_*-*-*_*h*min*s.json")
        ]
        
        for backup_type, pattern in backup_types:
            files = self._get_backup_files(backup_type, pattern)
            summary[backup_type] = len(files)
        
        return summary

class ConfigurationManager:
    """Gestionnaire de configuration centralis√©"""
    
    def __init__(self):
        self.API_URL = os.getenv('API_URL', 'http://localhost:5000/api')
        self.API_KEY = os.getenv('API_KEY', 'your-api-key-here')
        self.USE_API = os.getenv('USE_API', 'true').lower() == 'true'
        self.USE_LEGACY_FALLBACK = os.getenv('USE_LEGACY_FALLBACK', 'true').lower() == 'true'
        self.DEBUG = os.getenv('DEBUG', 'false').lower() == 'true'
        self.MAX_BACKUPS = int(os.getenv('MAX_BACKUPS', '3'))
        self.ENABLE_CROSS_VALIDATION = os.getenv('ENABLE_CROSS_VALIDATION', 'false').lower() == 'true'
        
        # Validation de la configuration
        if not self.USE_LEGACY_FALLBACK:
            print("‚ö†Ô∏è USE_LEGACY_FALLBACK=false - Le fallback Keras ne sera pas utilis√©")
        
        if not self.USE_API:
            print("‚ö†Ô∏è USE_API=false - L'API ne sera pas utilis√©e")

class APIClient:
    """Client API pour r√©cup√©rer les donn√©es d'entra√Ænement"""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'X-API-Key': config.API_KEY,
            'Content-Type': 'application/json'
        })
        self.logger = logging.getLogger(__name__)
    
    def test_connection(self) -> bool:
        """Test de connexion √† l'API"""
        try:
            response = self.session.get(
                f"{self.config.API_URL}/health",
                timeout=10,
                verify=False
            )
            return response.status_code == 200
        except Exception as e:
            if self.config.DEBUG:
                self.logger.debug(f"Erreur connexion API: {e}")
            return False
    
    def recuperer_toutes_connaissances(self) -> List[Dict[str, Any]]:
        """R√©cup√®re toutes les connaissances de la base de donn√©es"""
        self.logger.info("üîç R√©cup√©ration des connaissances depuis l'API...")
        
        toutes_connaissances = []
        connaissances_vues = set()
        
        # Requ√™tes larges pour couvrir le maximum de donn√©es
        requetes_recherche = [
            "bonjour", "salut", "hello", "comment", "que", "qui", "o√π", "quand",
            "pourquoi", "aide", "merci", "ai_licia", "ailicia", "alicia", "mila",
            "stream", "streaming", "TTS", "OBS", "configuration", "configurer",
            "utiliser", "plusieurs", "pc", "ordinateur", "audio", "voice", "vocal",
            "test", "erreur", "probl√®me", "solution", ""  # Requ√™te vide pour tout
        ]
        
        for i, requete in enumerate(requetes_recherche):
            self.logger.info(f"   üìã Recherche {i+1}/{len(requetes_recherche)}: '{requete}'...")
            
            try:
                payload = {
                    "query": requete,
                    "top_k": 1000,
                    "threshold": 0.0  # R√©cup√©rer tout
                }
                
                response = self.session.post(
                    f"{self.config.API_URL}/search",
                    json=payload,
                    timeout=30,
                    verify=False
                )
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get('success') and 'results' in data:
                        for resultat in data['results']:
                            # Cr√©er une cl√© unique pour √©viter les doublons
                            cle_unique = (
                                resultat.get('tag', '').strip(),
                                resultat.get('question', '').strip()
                            )
                            
                            if (cle_unique not in connaissances_vues and 
                                all(cle_unique) and 
                                len(resultat.get('question', '')) > 2 and
                                len(resultat.get('response', '')) > 2):
                                
                                connaissances_vues.add(cle_unique)
                                toutes_connaissances.append(resultat)
                        
                        self.logger.info(f"      ‚úÖ {len(data['results'])} r√©sultats trouv√©s")
                    else:
                        self.logger.warning(f"Pas de r√©sultats: {data}")
                else:
                    self.logger.warning(f"Erreur HTTP {response.status_code}")
                    
            except Exception as e:
                self.logger.error(f"Erreur requ√™te: {e}")
        
        self.logger.info(f"üìä Total connaissances uniques: {len(toutes_connaissances)}")
        return toutes_connaissances

class DataProcessor:
    """Processeur de donn√©es pour l'entra√Ænement"""
    
    def __init__(self, debug: bool = False):
        self.debug = debug
        self.lemmatizer = WordNetLemmatizer() if NLTK_AVAILABLE else None
        self.logger = logging.getLogger(__name__)
        
        # T√©l√©charger les ressources NLTK si n√©cessaire
        if NLTK_AVAILABLE:
            self._download_nltk_resources()
    
    def _download_nltk_resources(self):
        """T√©l√©charge les ressources NLTK n√©cessaires"""
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            self.logger.info("üì¶ T√©l√©chargement des ressources NLTK...")
            nltk.download('punkt', quiet=True)
            nltk.download('wordnet', quiet=True)
            nltk.download('omw-1.4', quiet=True)
    
    def nettoyer_texte(self, texte: str) -> str:
        """Nettoyage de texte optimis√© pour le domaine du streaming"""
        if not texte:
            return ""
        
        # Normalisation de base
        texte = texte.lower().strip()
        
        # Normalisation des termes sp√©cialis√©s
        termes_specialises = {
            'ai_licia': 'ailicia',
            'ai-licia': 'ailicia',
            'ai licia': 'ailicia',
            'tts': 'texttospeech',
            'text-to-speech': 'texttospeech',
            'text to speech': 'texttospeech',
            'obs': 'obscapture',
            'obs studio': 'obscapture',
            'plusieurs pc': 'plusieurspc',
            'multiples pc': 'plusieurspc',
            'en m√™me temps': 'simultanement',
            'm√™me temps': 'simultanement'
        }
        
        for ancien, nouveau in termes_specialises.items():
            texte = texte.replace(ancien, nouveau)
        
        # Suppression des caract√®res non pertinents
        import re
        texte = re.sub(r'[^\w\s]', ' ', texte)
        texte = re.sub(r'\s+', ' ', texte)
        
        return texte.strip()
    
    def convertir_donnees_vers_format_entrainement(
        self, 
        connaissances: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Convertit les donn√©es de la base vers le format d'entra√Ænement"""
        self.logger.info("üîÑ Conversion des donn√©es vers format d'entra√Ænement...")
        
        # Grouper par tag
        donnees_par_tag = {}
        
        for connaissance in connaissances:
            tag = connaissance.get('tag', 'general').strip()
            question = self.nettoyer_texte(connaissance.get('question', ''))
            reponse = connaissance.get('response', '').strip()
            
            # Validation
            if not tag or not question or not reponse:
                continue
            
            if len(question) < 3 or len(reponse) < 3:
                continue
            
            if tag not in donnees_par_tag:
                donnees_par_tag[tag] = {
                    'patterns': [],
                    'responses': []
                }
            
            # Ajouter si pas d√©j√† pr√©sent
            if question not in donnees_par_tag[tag]['patterns']:
                donnees_par_tag[tag]['patterns'].append(question)
            
            if reponse not in donnees_par_tag[tag]['responses']:
                donnees_par_tag[tag]['responses'].append(reponse)
        
        # Filtrer les tags avec donn√©es insuffisantes
        donnees_valides = {}
        for tag, donnees in donnees_par_tag.items():
            if len(donnees['patterns']) > 0 and len(donnees['responses']) > 0:
                donnees_valides[tag] = donnees
            elif self.debug:
                self.logger.warning(f"Tag '{tag}' ignor√© (donn√©es insuffisantes)")
        
        # Statistiques
        total_patterns = sum(len(d['patterns']) for d in donnees_valides.values())
        total_responses = sum(len(d['responses']) for d in donnees_valides.values())
        
        self.logger.info(f"üìã {len(donnees_valides)} tags valides")
        self.logger.info(f"üìä {total_patterns} patterns, {total_responses} responses")
        
        return donnees_valides
    
    def augmenter_donnees(self, donnees_tags: Dict[str, Any]) -> Dict[str, Any]:
        """Augmentation intelligente des donn√©es d'entra√Ænement"""
        self.logger.info("üîÑ Augmentation des donn√©es d'entra√Ænement...")
        
        # Synonymes contextuels pour le domaine
        synonymes = {
            'comment': ['de quelle mani√®re', 'comment faire', 'comment puis-je'],
            'configurer': ['param√©trer', 'r√©gler', 'ajuster', 'installer'],
            'utiliser': ['employer', 'se servir de', 'faire fonctionner'],
            'ailicia': ['ia', 'assistant', 'bot', 'chatbot'],
            'plusieurs': ['multiples', 'diff√©rents', 'nombreux'],
            'ordinateur': ['pc', 'machine', 'poste'],
            'simultanement': ['en parall√®le', 'conjointement'],
            'aide': ['assistance', 'support', 'aider'],
            'probl√®me': ['souci', 'erreur', 'bug', 'dysfonctionnement']
        }
        
        donnees_augmentees = {}
        
        for tag, donnees in donnees_tags.items():
            nouvelles_donnees = {
                'patterns': donnees['patterns'].copy(),
                'responses': donnees['responses'].copy()
            }
            
            # Cr√©er des variations avec synonymes
            patterns_originaux = nouvelles_donnees['patterns'].copy()
            for pattern in patterns_originaux:
                mots = pattern.split()
                
                # Appliquer les synonymes (maximum 2 par mot)
                for i, mot in enumerate(mots):
                    if mot in synonymes:
                        for synonyme in synonymes[mot][:2]:
                            nouveaux_mots = mots.copy()
                            nouveaux_mots[i] = synonyme
                            nouveau_pattern = ' '.join(nouveaux_mots)
                            
                            if nouveau_pattern not in nouvelles_donnees['patterns']:
                                nouvelles_donnees['patterns'].append(nouveau_pattern)
            
            donnees_augmentees[tag] = nouvelles_donnees
        
        # Statistiques de l'augmentation
        patterns_avant = sum(len(d['patterns']) for d in donnees_tags.values())
        patterns_apr√®s = sum(len(d['patterns']) for d in donnees_augmentees.values())
        
        self.logger.info(f"üìà Patterns avant: {patterns_avant}, apr√®s: {patterns_apr√®s}")
        self.logger.info(f"üéØ Augmentation: +{patterns_apr√®s - patterns_avant} patterns")
        
        return donnees_augmentees

class ModelTrainer:
    """Entra√Æneur de mod√®le Keras optimis√©"""
    
    def __init__(self, config: ConfigurationManager):
        self.config = config
        self.lemmatizer = WordNetLemmatizer() if NLTK_AVAILABLE else None
        self.logger = logging.getLogger(__name__)
    
    def entrainer_modele(
        self, 
        donnees_tags: Dict[str, Any]
    ) -> Tuple[List[str], List[str], Optional[object], TrainingMetrics]:
        """Entra√Ænement du mod√®le Keras optimis√© avec m√©triques"""
        
        if not TENSORFLOW_AVAILABLE:
            self.logger.error("TensorFlow non disponible - impossible d'entra√Æner le mod√®le")
            return [], [], None, TrainingMetrics(start_time=time.time())
        
        # Initialisation des m√©triques
        metrics = TrainingMetrics(start_time=time.time())
        
        self.logger.info("ü§ñ Entra√Ænement du mod√®le Keras...")
        
        # Pr√©paration des donn√©es
        words = []
        classes = []
        documents = []
        ignore_words = ['?', '.', ',', '!', ':', ';', '(', ')', '[', ']', '"', "'"]
        
        # Extraction des features
        for tag, donnees in donnees_tags.items():
            for pattern in donnees['patterns']:
                if NLTK_AVAILABLE:
                    word_list = word_tokenize(pattern, language='french')
                else:
                    word_list = pattern.split()
                
                words.extend(word_list)
                documents.append((word_list, tag))
            
            if tag not in classes:
                classes.append(tag)
        
        # Nettoyage et lemmatisation
        if self.lemmatizer:
            words_cleaned = [
                self.lemmatizer.lemmatize(w.lower()) 
                for w in words 
                if w not in ignore_words and len(w) > 1 and w.isalpha()
            ]
        else:
            words_cleaned = [
                w.lower() 
                for w in words 
                if w not in ignore_words and len(w) > 1 and w.isalpha()
            ]
        
        # Filtrage des mots rares (sauf mots-cl√©s importants)
        mots_cles = ['ailicia', 'texttospeech', 'obscapture', 'simultanement', 'plusieurspc']
        word_freq = Counter(words_cleaned)
        words_filtered = [
            word for word, freq in word_freq.items()
            if freq >= 2 or word in mots_cles
        ]
        
        words = sorted(list(set(words_filtered)))
        classes = sorted(list(set(classes)))
        
        # Mise √† jour des m√©triques
        metrics.vocabulary_size = len(words)
        metrics.valid_tags = len(classes)
        metrics.total_patterns = sum(len(d['patterns']) for d in donnees_tags.values())
        
        self.logger.info(f"üìä Vocabulaire: {len(words)} mots")
        self.logger.info(f"üìä Classes: {len(classes)} tags")
        self.logger.info(f"üìä Documents: {len(documents)} exemples")
        
        # Cr√©ation des donn√©es d'entra√Ænement
        training_data = []
        output_empty = [0] * len(classes)
        
        for doc in documents:
            bag = []
            pattern_words = doc[0]
            
            if self.lemmatizer:
                pattern_words = [self.lemmatizer.lemmatize(w.lower()) for w in pattern_words]
            else:
                pattern_words = [w.lower() for w in pattern_words]
            
            # Bag of words avec pond√©ration
            for w in words:
                count = pattern_words.count(w)
                if count > 0:
                    # Boost pour les mots-cl√©s importants
                    if w in mots_cles:
                        bag.append(min(1.5, count * 0.8))
                    else:
                        bag.append(min(1.0, count * 0.6))
                else:
                    bag.append(0.0)
            
            # Output vector
            output_row = list(output_empty)
            output_row[classes.index(doc[1])] = 1
            training_data.append([bag, output_row])
        
        # Pr√©paration finale des donn√©es
        random.shuffle(training_data)
        train_x = np.array([item[0] for item in training_data], dtype=np.float32)
        train_y = np.array([item[1] for item in training_data], dtype=np.float32)
        
        self.logger.info(f"üèãÔ∏è Donn√©es pr√©par√©es: X={train_x.shape}, Y={train_y.shape}")
        
        # Construction du mod√®le optimis√©
        model = Sequential([
            Input(shape=(len(train_x[0]),)),
            Dense(512, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.4),
            
            Dense(256, activation='relu', kernel_regularizer=l2(0.001)),
            BatchNormalization(),
            Dropout(0.3),
            
            Dense(128, activation='relu'),
            Dropout(0.2),
            
            Dense(64, activation='relu'),
            Dropout(0.1),
            
            Dense(len(train_y[0]), activation='softmax')
        ])
        
        # Compilation
        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        model.compile(
            loss='categorical_crossentropy',
            optimizer=optimizer,
            metrics=['accuracy']
        )
        
        # Callbacks pour un entra√Ænement optimal
        callbacks = [
            EarlyStopping(
                monitor='val_accuracy',
                patience=50,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=20,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        self.logger.info("üöÄ Lancement de l'entra√Ænement...")
        
        # Entra√Ænement
        history = model.fit(
            train_x, train_y,
            epochs=300,
            batch_size=8,
            verbose=1,
            validation_split=0.2,
            callbacks=callbacks
        )
        
        # Mise √† jour des m√©triques finales
        final_loss, final_accuracy = model.evaluate(train_x, train_y, verbose=0)
        metrics.model_accuracy = final_accuracy
        metrics.training_loss = final_loss
        metrics.epochs_completed = len(history.history['loss'])
        metrics.end_time = time.time()
        
        self.logger.info("‚úÖ Entra√Ænement termin√©")
        self.logger.info(f"üìä Pr√©cision finale: {final_accuracy:.4f}")
        self.logger.info(f"üìä Perte finale: {final_loss:.4f}")
        self.logger.info(f"üìä Epochs: {metrics.epochs_completed}")
        self.logger.info(f"üìä Dur√©e: {metrics.duration:.2f}s")
        
        return words, classes, model, metrics

def main():
    """Fonction principale d'entra√Ænement am√©lior√©e"""
    print("=" * 90)
    print("üöÄ MILA ASSIST - ENTRA√éNEMENT VERSION 2.0 (GESTION BACKUPS INTELLIGENTE)")
    print("=" * 90)
    print("üìö Concepteur D√©veloppeur d'Applications - Niveau 6")
    print("üéØ Entra√Ænement du mod√®le Keras depuis la base de donn√©es")
    print("‚ö° Mise √† jour automatique des fichiers avec gestion intelligente des backups")
    print("üóëÔ∏è Suppression automatique des anciennes versions (limite: 3 backups)")
    print()
    
    # V√©rification des d√©pendances
    if not TENSORFLOW_AVAILABLE:
        print("‚ùå TensorFlow requis pour l'entra√Ænement")
        print("üí° Installez avec: pip install tensorflow")
        return False
    
    if not NLTK_AVAILABLE:
        print("‚ö†Ô∏è NLTK non disponible - utilisation de la tokenisation simple")
    
    try:
        # Initialisation des composants
        config = ConfigurationManager()
        
        # Configuration du logging
        training_logger = TrainingLogger()
        logger = logging.getLogger(__name__)
        
        # Initialisation des gestionnaires
        api_client = APIClient(config)
        data_processor = DataProcessor(debug=config.DEBUG)
        model_trainer = ModelTrainer(config)
        backup_manager = BackupManager(
            os.path.dirname(os.path.abspath(__file__)),
            max_backups=config.MAX_BACKUPS
        )
        
        logger.info(f"üîß Configuration:")
        logger.info(f"   - API URL: {config.API_URL}")
        logger.info(f"   - Utiliser API: {config.USE_API}")
        logger.info(f"   - Fallback activ√©: {config.USE_LEGACY_FALLBACK}")
        logger.info(f"   - Mode debug: {config.DEBUG}")
        logger.info(f"   - Max backups: {config.MAX_BACKUPS}")
        logger.info(f"   - Validation crois√©e: {config.ENABLE_CROSS_VALIDATION}")
        
        if not config.USE_LEGACY_FALLBACK:
            logger.warning("Le fallback Keras est d√©sactiv√© - entra√Ænement tout de m√™me effectu√©")
        
        # Affichage de l'√©tat des backups
        backup_summary = backup_manager.get_backup_summary()
        print("\nüìä √âtat actuel des backups:")
        for backup_type, count in backup_summary.items():
            print(f"   - {backup_type}: {count} fichiers")
        print()
        
        # Test de connexion API
        if config.USE_API:
            logger.info("üîç Test de connexion √† l'API...")
            if api_client.test_connection():
                logger.info("‚úÖ API accessible")
            else:
                logger.error("‚ùå API non accessible")
                logger.error("üí° V√©rifiez que l'API fonctionne sur le NAS")
                return False
        else:
            logger.warning("API d√©sactiv√©e dans la configuration")
            return False
        
        # Initialisation des m√©triques
        metrics = TrainingMetrics(start_time=time.time())
        
        # R√©cup√©ration des donn√©es
        connaissances = api_client.recuperer_toutes_connaissances()
        if not connaissances:
            logger.error("Aucune donn√©e r√©cup√©r√©e - impossible d'entra√Æner")
            return False
        
        metrics.total_knowledge = len(connaissances)
        
        # Traitement des donn√©es
        donnees_tags = data_processor.convertir_donnees_vers_format_entrainement(connaissances)
        donnees_augmentees = data_processor.augmenter_donnees(donnees_tags)
        
        # Calcul du facteur d'augmentation
        patterns_avant = sum(len(d['patterns']) for d in donnees_tags.values())
        patterns_apr√®s = sum(len(d['patterns']) for d in donnees_augmentees.values())
        metrics.data_augmentation_factor = patterns_apr√®s / patterns_avant if patterns_avant > 0 else 1.0
        
        # Sauvegarde des fichiers existants
        logger.info("üíæ Sauvegarde des fichiers existants...")
        backup_manager.backup_model()
        backup_manager.backup_vocabulary_files()
        
        # Entra√Ænement du mod√®le
        words, classes, model, training_metrics = model_trainer.entrainer_modele(donnees_augmentees)
        
        if not words or not classes:
            logger.error("Erreur lors de l'entra√Ænement")
            return False
        
        # Fusion des m√©triques
        metrics.end_time = training_metrics.end_time
        metrics.valid_tags = training_metrics.valid_tags
        metrics.vocabulary_size = training_metrics.vocabulary_size
        metrics.total_patterns = training_metrics.total_patterns
        metrics.model_accuracy = training_metrics.model_accuracy
        metrics.training_loss = training_metrics.training_loss
        metrics.epochs_completed = training_metrics.epochs_completed
        
        # Sauvegarde des nouveaux fichiers
        logger.info("üíæ Sauvegarde des nouveaux fichiers...")
        
        # Sauvegarde du mod√®le
        if model is not None:
            model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chatbot_model.keras")
            model.save(model_path)
            logger.info(f"üíæ Nouveau mod√®le sauvegard√©: {model_path}")
        
        # Sauvegarde des vocabulaires
        base_dir = os.path.dirname(os.path.abspath(__file__))
        for filename, data in [("words.pkl", words), ("classes.pkl", classes)]:
            file_path = os.path.join(base_dir, filename)
            with open(file_path, 'wb') as f:
                pickle.dump(data, f)
            logger.info(f"üìÑ Nouveau {filename} cr√©√©")
        
        # Sauvegarde des patterns et m√©triques
        backup_manager.backup_training_patterns(donnees_augmentees)
        backup_manager.backup_metrics(metrics)
        
        # Sauvegarde des m√©triques dans l'historique
        training_logger.log_metrics(metrics)
        
        # R√©sum√© final des backups apr√®s nettoyage
        final_backup_summary = backup_manager.get_backup_summary()
        
        # R√©sum√© final
        print("\n" + "=" * 90)
        print("‚úÖ ENTRA√éNEMENT VERSION 2.0 TERMIN√â AVEC SUCC√àS!")
        print("=" * 90)
        print(f"üìä Connaissances utilis√©es: {metrics.total_knowledge}")
        print(f"üìä Tags d'entra√Ænement: {metrics.valid_tags}")
        print(f"üìä Vocabulaire: {metrics.vocabulary_size} mots")
        print(f"üìä Patterns total: {metrics.total_patterns}")
        print(f"üìä Facteur d'augmentation: {metrics.data_augmentation_factor:.2f}x")
        print(f"üìä Pr√©cision finale: {metrics.model_accuracy:.4f}")
        print(f"üìä Dur√©e totale: {metrics.duration:.2f} secondes")
        print(f"üìä Epochs compl√©t√©s: {metrics.epochs_completed}")
        
        print("\nüìÅ Fichiers mis √† jour:")
        print("   ‚úÖ chatbot_model.keras (mod√®le Keras entra√Æn√©)")
        print("   ‚úÖ words.pkl (vocabulaire √† jour)")
        print("   ‚úÖ classes.pkl (classes √† jour)")
        print("   ‚úÖ training_patterns.pkl (patterns de fallback)")
        
        print("\nüóÉÔ∏è Gestion des backups (max 3 versions):")
        for backup_type, count in final_backup_summary.items():
            print(f"   üì¶ {backup_type}: {count} versions conserv√©es")
        
        print("\nüí° Le chatbot peut maintenant utiliser le mod√®le mis √† jour!")
        print("üîÑ Red√©marrez l'application pour prendre en compte les changements")
        
        return True
        
    except KeyboardInterrupt:
        print("\nüõë Entra√Ænement interrompu par l'utilisateur")
        return False
    except Exception as e:
        logger = logging.getLogger(__name__)
        logger.error(f"Erreur lors de l'entra√Ænement: {e}")
        if hasattr(config, 'DEBUG') and config.DEBUG:
            import traceback
            traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
