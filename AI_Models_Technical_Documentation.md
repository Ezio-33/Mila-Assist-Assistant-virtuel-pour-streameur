# Documentation Technique - Mod√®les IA Mila-Assist

## üß† Vue d'ensemble de l'architecture IA

Mila-Assist utilise une **architecture hybride multi-mod√®les** optimis√©e pour des contraintes de ressources limit√©es (CPU-only, <4GB RAM) tout en maintenant une qualit√© de r√©ponse √©lev√©e.

## üìã Inventaire des mod√®les IA

### 1. Mod√®le Principal: chatbot_model.keras

**Type**: R√©seau de neurones feed-forward pour classification d'intentions  
**Framework**: TensorFlow/Keras 2.7.0  
**Fichier**: `chatbot_model.keras` (330 KB)  
**R√¥le**: Syst√®me de fallback intelligent pour classification d'intentions  

#### Architecture d√©taill√©e:
```python
model = Sequential([
    Input(shape=(len(vocabulary),)),           # ~800 features (vocabulaire)
    Dense(128, activation='relu'),             # Couche cach√©e 1
    Dropout(0.5),                             # R√©gularisation
    Dense(64, activation='relu'),              # Couche cach√©e 2  
    Dropout(0.5),                             # R√©gularisation
    Dense(45, activation='softmax')            # Classification 45 intents
])
```

#### Param√®tres d'entra√Ænement:
- **Optimiseur**: SGD avec momentum (0.9) et Nesterov
- **Learning Rate**: D√©croissance exponentielle (0.01 ‚Üí 0.96^epoch)
- **Loss**: Categorical Crossentropy
- **Epochs**: 200
- **Batch Size**: 5
- **Donn√©es**: ~300 patterns d'entra√Ænement sur 45 classes

#### Preprocessing:
1. **Tokenisation**: NLTK word_tokenize
2. **Lemmatisation**: WordNetLemmatizer (anglais/fran√ßais)
3. **Vectorisation**: Bag-of-Words binaire
4. **Vocabulaire**: Sauvegard√© dans `words.pkl` (~800 mots uniques)
5. **Classes**: Sauvegard√©es dans `classes.pkl` (45 intents)

#### Performance:
- **Seuil de confiance**: 0.25 (25%)
- **Latence**: < 50ms sur CPU i5
- **M√©moire**: ~10MB charg√© en RAM
- **Accuracy**: ~85% sur donn√©es de validation

#### Utilisation dans le code:
```python
# Chargement (app_v2.py ligne 58-61)
model_path = os.path.join(BASE_DIR, "chatbot_model.keras")
if os.path.exists(model_path):
    model = load_model(model_path)

# Pr√©diction (app_v2.py ligne 359-374)  
def predict_class(sentence):
    p = bow(sentence, words, show_details=False)
    res = model.predict(np.array([p]))[0]
    ERROR_THRESHOLD = 0.25
    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]
    return [{"intent": classes[r[0]], "probability": str(r[1])} for r in results]
```

### 2. Transformer: paraphrase-multilingual-MiniLM-L12-v2

**Type**: Sentence Transformer multilingue  
**Base**: Microsoft MiniLM-L12  
**Taille**: ~130 MB  
**R√¥le**: Reformulation contextuelle avanc√©e (mode "natural")  

#### Sp√©cifications techniques:
- **Architecture**: 12 couches Transformer
- **Attention heads**: 12
- **Hidden size**: 384
- **Vocabulaire**: 250k tokens (multilingue)
- **Langues support√©es**: 50+ dont fran√ßais optimis√©
- **Max sequence length**: 128 tokens

#### Utilisation:
```python
# Chargement (response_modes.py ligne 97-100)
from sentence_transformers import SentenceTransformer
self.sentence_model = SentenceTransformer(
    'paraphrase-multilingual-MiniLM-L12-v2'
)
```

#### Fonctionnalit√©s:
1. **Reformulation contextuelle**: Am√©lioration des r√©ponses brutes
2. **M√©moire conversationnelle**: Continuit√© entre √©changes
3. **D√©tection de similarit√©**: Comparaison s√©mantique entre questions
4. **Templates intelligents**: Adaptation du style selon le contexte

### 3. Vectorisation TF-IDF

**Type**: Term Frequency - Inverse Document Frequency  
**Impl√©mentation**: scikit-learn TfidfVectorizer  
**R√¥le**: Recherche s√©mantique primaire (niveau 1)  

#### Configuration:
```python
# Vectorisation (database.py)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

vectorizer = TfidfVectorizer(
    max_features=1000,
    stop_words=None,  # Personnalis√© pour fran√ßais
    ngram_range=(1, 2)  # Unigrammes + bigrammes
)
```

#### Performance:
- **Latence**: 10-30ms par requ√™te
- **Seuil similarit√©**: 0.7 (70%)
- **Stockage**: Base SQLite avec index optimis√©s
- **Couverture**: ~80% des requ√™tes r√©solues au niveau 1

## üîÑ Pipeline de traitement hybride

### Cascade intelligente:
```
1. üìä TF-IDF Vectoriel (Primaire)
   ‚îú‚îÄ Similarit√© ‚â• 0.7 ‚Üí ‚úÖ R√©ponse directe
   ‚îî‚îÄ √âchec ‚Üí Niveau 2

2. üß† Mod√®le Keras (Fallback)  
   ‚îú‚îÄ Confiance ‚â• 0.25 ‚Üí ‚úÖ Classification intent
   ‚îî‚îÄ √âchec ‚Üí Niveau 3

3. üí¨ R√©ponse par d√©faut
   ‚îî‚îÄ ‚úÖ Message g√©n√©rique

4. üé® Reformulation (Post-processing)
   ‚îú‚îÄ MINIMAL: R√©ponse brute
   ‚îú‚îÄ BALANCED: Templates + variations  
   ‚îî‚îÄ NATURAL: Sentence Transformer + contexte
```

### Optimisations de performance:
1. **D√©tection automatique de mode**: Selon RAM disponible
2. **Cache embeddings**: √âvite recalculs co√ªteux
3. **Lazy loading**: Chargement √† la demande des mod√®les lourds
4. **Fallback graceful**: Jamais d'√©chec total

## üìà M√©triques et monitoring

### KPIs par mod√®le:
```python
# M√©triques collect√©es en temps r√©el
{
    "tfidf": {
        "latency_ms": 25,
        "coverage_rate": 0.78,
        "avg_similarity": 0.82
    },
    "keras": {
        "latency_ms": 45,
        "usage_rate": 0.15,
        "avg_confidence": 0.67
    },
    "sentence_transformer": {
        "latency_ms": 350,
        "usage_rate": 0.12,
        "enhancement_applied": 0.45
    }
}
```

### Logs de performance:
```python
# Exemple de log (app_v2.py)
logger.info(f"R√©ponse trouv√©e en DB (similarit√©: {best_match['similarity']:.2f})")
logger.info("R√©ponse du syst√®me legacy")
logger.warning(f"sentence-transformers non disponible ({e})")
```

## üîß Configuration et d√©ploiement

### Variables d'environnement:
```bash
# Mode de r√©ponse IA  
RESPONSE_MODE=balanced  # minimal|balanced|natural

# Seuils de performance
TFIDF_THRESHOLD=0.7
KERAS_THRESHOLD=0.25

# Optimisations
SENTENCE_TRANSFORMER_CACHE=true
MODEL_PRELOAD=true
```

### Ressources syst√®me:
```yaml
Minimal (TF-IDF only):
  RAM: ~50MB
  CPU: <10% sur i5
  Latence: <30ms

Balanced (TF-IDF + Keras):
  RAM: ~100MB  
  CPU: <20% sur i5
  Latence: <100ms

Natural (Full stack):
  RAM: ~200MB
  CPU: <30% sur i5  
  Latence: <400ms
```

## üîç Justifications techniques

### Pourquoi cette architecture ?

1. **Contraintes mat√©rielles**: NAS Synology, VPS basiques
2. **Contraintes budget**: Pas d'API externe payante
3. **Contraintes latence**: < 500ms pour UX streameur
4. **Contraintes qualit√©**: R√©ponses coh√©rentes et contr√¥lables

### Alternatives consid√©r√©es et rejet√©es:

#### ‚ùå LLM complets (GPT, Claude):
- **Pro**: Qualit√© excellente
- **Con**: Co√ªt prohibitif, latence r√©seau, d√©pendance externe

#### ‚ùå BERT-Large, CamemBERT:
- **Pro**: Pr√©cision √©lev√©e  
- **Con**: 1.3GB RAM, 2-5s latence, GPU requis

#### ‚ùå Mod√®les locaux (Llama, Mistral):
- **Pro**: Contr√¥le total
- **Con**: >8GB RAM, GPU requis, complexit√© d√©ploiement

### ‚úÖ Solution retenue:
**Cascade TF-IDF ‚Üí Keras ‚Üí MiniLM** offre le meilleur compromis:
- Performance acceptable sur hardware limit√©
- Qualit√© suffisante pour cas d'usage streameur  
- Co√ªt op√©rationnel minimal
- √âvolutivit√© vers mod√®les plus avanc√©s

## üõ†Ô∏è Am√©lioration continue

### Feedback loop:
1. **Collecte**: √âvaluations utilisateur via UI/API
2. **Analyse**: Identification patterns d'√©chec  
3. **Enrichissement**: Ajout donn√©es d'entra√Ænement
4. **R√©entra√Ænement**: P√©riodique des mod√®les
5. **A/B Testing**: Validation am√©lioration

### Roadmap IA:
- **Court terme**: Fine-tuning seuils + cache optimisation
- **Moyen terme**: Ensemble methods, custom embeddings
- **Long terme**: Migration vers mod√®les plus avanc√©s si ressources

Cette architecture d√©montre une **approche pragmatique de l'IA en production**, privil√©giant la robustesse et l'efficacit√© aux performances acad√©miques maximales.